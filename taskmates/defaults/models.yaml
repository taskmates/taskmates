# Model configurations for Taskmates
# Structure:
#   metadata: Taskmates-specific settings (not passed to LLM client)
#   client: LLM client configuration
#     type: Full import path to the client class
#     kwargs: Arguments passed directly to the client constructor

default:
  metadata:
    max_context_window: 200000
    max_output_tokens: 64000
  client:
    type: langchain_anthropic.ChatAnthropic
    kwargs:
      model: claude-sonnet-4-5
      temperature: 0.2

gpt-5:
  metadata:
    max_context_window: 400000
  client:
    type: langchain_openai.ChatOpenAI
    kwargs:
      model: gpt-5

gpt-5-mini:
  metadata:
    max_context_window: 400000
  client:
    type: langchain_openai.ChatOpenAI
    kwargs:
      model: gpt-5-mini

gpt-5-nano:
  metadata:
    max_context_window: 400000
  client:
    type: langchain_openai.ChatOpenAI
    kwargs:
      model: gpt-5-nano

gpt-5-chat-latest:
  metadata:
    max_context_window: 400000
  client:
    type: langchain_openai.ChatOpenAI
    kwargs:
      model: gpt-5-chat-latest

gpt-4.1:
  metadata:
    max_context_window: 1000000
  client:
    type: langchain_openai.ChatOpenAI
    kwargs:
      model: gpt-4.1
      temperature: 0.2

gpt-4o:
  metadata:
    max_context_window: 128000
  client:
    type: langchain_openai.ChatOpenAI
    kwargs:
      model: gpt-4o
      openai_api_base: https://api.openai.com/v1
      temperature: 0.2

gpt-4o-mini:
  metadata:
    max_context_window: 128000
  client:
    type: langchain_openai.ChatOpenAI
    kwargs:
      model: gpt-4o-mini
      openai_api_base: https://api.openai.com/v1
      temperature: 0.2

claude-haiku-4-5:
  metadata:
    max_context_window: 200000
    max_output_tokens: 64000
  client:
    type: langchain_anthropic.ChatAnthropic
    kwargs:
      model: claude-haiku-4-5
      temperature: 0.2

claude-opus-4-1:
  metadata:
    max_context_window: 200000
    max_output_tokens: 64000
  client:
    type: langchain_anthropic.ChatAnthropic
    kwargs:
      model: claude-opus-4-1
      temperature: 0.2


claude-sonnet-4-5:
  metadata:
    max_context_window: 200000
    max_output_tokens: 64000
  client:
    type: langchain_anthropic.ChatAnthropic
    kwargs:
      model: claude-sonnet-4-5
      temperature: 0.2

grok-4:
  metadata:
    max_context_window: 256000
  client:
    type: langchain_xai.ChatXAI
    kwargs:
      model: grok-4-latest
      temperature: 0.2
      api_key: env:XAI_API_KEY

grok-4-fast-reasoning:
  metadata:
    max_context_window: 2000000
  client:
    type: langchain_xai.ChatXAI
    kwargs:
      model: grok-4-fast-reasoning
      temperature: 0.2
      api_key: env:XAI_API_KEY

grok-4-fast-non-reasoning:
  metadata:
    max_context_window: 2000000
  client:
    type: langchain_xai.ChatXAI
    kwargs:
      model: grok-4-fast-non-reasoning
      temperature: 0.2
      api_key: env:XAI_API_KEY

gemini-2.5-pro:
  metadata:
    max_context_window: 2000000
  client:
    type: langchain_google_genai.ChatGoogleGenerativeAI
    kwargs:
      model: gemini-2.5-pro
      temperature: 0.2

gemini-2.5-flash:
  metadata:
    max_context_window: 1000000
  client:
    type: langchain_google_genai.ChatGoogleGenerativeAI
    kwargs:
      model: gemini-2.5-flash
      temperature: 0.2

gemini-2.5-flash-lite:
  metadata:
    max_context_window: 1000000
  client:
    type: langchain_google_genai.ChatGoogleGenerativeAI
    kwargs:
      model: gemini-2.5-flash-lite-preview-06-17
      temperature: 0.2

mixtral-8x7b-32768:
  metadata:
    max_context_window: 32768
  client:
    type: langchain_openai.ChatOpenAI
    kwargs:
      model: mixtral-8x7b-32768
      openai_api_base: https://api.groq.com/openai/v1
      temperature: 0.2
      api_key: env:GROQ_API_KEY

echo:
  metadata:
    max_context_window: 64000
  client:
    type: taskmates.core.workflows.markdown_completion.completions.llm_completion.testing.echo.Echo
    kwargs:
      model: echo

quote:
  metadata:
    max_context_window: 64000
  client:
    type: taskmates.core.workflows.markdown_completion.completions.llm_completion.testing.quote.Quote
    kwargs:
      model: quote

fixture:
  metadata:
    max_context_window: 4096
  client:
    type: taskmates.core.workflows.markdown_completion.completions.llm_completion.testing.fixture_chat_model.FixtureChatModel
    kwargs:
      model: fixture

codeqwen:
  metadata:
    max_context_window: 32768
  client:
    type: langchain_ollama.ChatOllama
    kwargs:
      model: qwen2.5-coder:32b
      temperature: 0.2
      base_url: http://localhost:11434

codellama:
  metadata:
    max_context_window: 2048
  client:
    type: langchain_ollama.ChatOllama
    kwargs:
      model: codellama:70b
      temperature: 0.2
      base_url: http://localhost:11434

deepseek-r1:
  metadata:
    max_context_window: 131072
  client:
    type: langchain_ollama.ChatOllama
    kwargs:
      model: deepseek-r1:70b
      temperature: 0.2
      base_url: http://localhost:11434

llama4:
  metadata:
    max_context_window: 10485760
  client:
    type: langchain_ollama.ChatOllama
    kwargs:
      model: llama4:latest
      temperature: 0.2
      base_url: http://localhost:11434

llava:
  metadata:
    max_context_window: 4096
  client:
    type: langchain_ollama.ChatOllama
    kwargs:
      model: llava:34b
      temperature: 0.2
      base_url: http://localhost:11434

llava-phi3:
  metadata:
    max_context_window: 4096
  client:
    type: langchain_ollama.ChatOllama
    kwargs:
      model: llava-phi3:latest
      temperature: 0.2
      base_url: http://localhost:11434

minicpm-v:
  metadata:
    max_context_window: 32768
  client:
    type: langchain_ollama.ChatOllama
    kwargs:
      model: minicpm-v:latest
      temperature: 0.2
      base_url: http://localhost:11434

moondream:
  metadata:
    max_context_window: 2048
  client:
    type: langchain_ollama.ChatOllama
    kwargs:
      model: moondream:latest
      temperature: 0.2
      base_url: http://localhost:11434

phi4:
  metadata:
    max_context_window: 16384
  client:
    type: langchain_ollama.ChatOllama
    kwargs:
      model: phi4:latest
      temperature: 0.2
      base_url: http://localhost:11434

qwen2.5:
  metadata:
    max_context_window: 32768
  client:
    type: langchain_ollama.ChatOllama
    kwargs:
      model: qwen2.5:72b
      temperature: 0.2
      base_url: http://localhost:11434

qwen3-vl:32b:
  metadata:
    max_context_window: 32768
  client:
    type: langchain_ollama.ChatOllama
    kwargs:
      model: qwen3-vl:32b
      temperature: 0.2
      base_url: http://localhost:11434

qwen2.5-coder:
  metadata:
    max_context_window: 32768
  client:
    type: langchain_ollama.ChatOllama
    kwargs:
      model: qwen2.5-coder:32b
      temperature: 0.2
      base_url: http://localhost:11434

qwen3:
  metadata:
    max_context_window: 40960
  client:
    type: langchain_ollama.ChatOllama
    kwargs:
      model: qwen3:32b
      temperature: 0.2
      base_url: http://localhost:11434

qwq:
  metadata:
    max_context_window: 131072
  client:
    type: langchain_ollama.ChatOllama
    kwargs:
      model: qwq:latest
      temperature: 0.2
      base_url: http://localhost:11434

gpt-oss-20b:
  metadata:
    max_context_window: 131072
  client:
    type: langchain_ollama.ChatOllama
    kwargs:
      model: gpt-oss:20b
      base_url: http://localhost:11434
      temperature: 0.2
      reasoning: true
      reasoning_effort: high

gpt-oss-120b:
  metadata:
    max_context_window: 131072
  client:
    type: langchain_ollama.ChatOllama
    kwargs:
      model: gpt-oss:120b
      base_url: http://localhost:11434
      temperature: 0.2
      reasoning: true
      reasoning_effort: high
